{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev_pid_1</td>\n",
       "      <td>I enjoyed today, and I still am! Tomorrows dep...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev_pid_2</td>\n",
       "      <td>I sorta tried to kill myself : I had a total b...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev_pid_3</td>\n",
       "      <td>Best suicide method? : I like it quick and eas...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dev_pid_4</td>\n",
       "      <td>a story : I remember the time I'd get on my 3D...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev_pid_5</td>\n",
       "      <td>The world only cares about beautiful people : ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PID                                             Review     Label\n",
       "0  dev_pid_1  I enjoyed today, and I still am! Tomorrows dep...  moderate\n",
       "1  dev_pid_2  I sorta tried to kill myself : I had a total b...  moderate\n",
       "2  dev_pid_3  Best suicide method? : I like it quick and eas...  moderate\n",
       "3  dev_pid_4  a story : I remember the time I'd get on my 3D...  moderate\n",
       "4  dev_pid_5  The world only cares about beautiful people : ...  moderate"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/home/redleaf/Documents/DUK/NLP/NLP_PROJECT_DEPRESSION/dev_with_labels.tsv\", delimiter='\\t')\n",
    "df = df.rename(columns={'Text data':'Review'}, inplace=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4496 entries, 0 to 4495\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   PID     4496 non-null   object\n",
      " 1   Review  4496 non-null   object\n",
      " 2   Label   4496 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 105.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PID       0\n",
       "Review    0\n",
       "Label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "moderate          2306\n",
       "not depression    1830\n",
       "severe             360\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.Review\n",
    "Y = df.Label\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoyed today, and I still am! Tomorrows depression can wait! : Today, I'm warm and cozy :) I even had some tea, to complement my sandwich!\n",
      "This is a crazy time we live in, but it's only the beginning, with the world going to shambles, being alone feels werid.\n",
      "But a stranger came through and saved me,\n",
      "Of course it's a debt I'll have to repay, but I'm grateful to have this debt!\n",
      "Crazy what a few dollars can do to change someones situation.\n",
      "But these days, it's all about here and now!\n",
      "I don't have the ability to relax just because, I'm full, clean, and warm.\n",
      "Tomorrow it's back to reality! \n",
      "The motel was just to get ourself together! We been out here panicking the past two weeks and the last 3 days exspecially.\n",
      "I got blessed this week that's for sure!\n",
      "Two weeks till payday.\n",
      "What to do, what to do.\n",
      "Well I can't just pull another loan, so I gotta make what's left it last. \n",
      "Planet fitness anyone???!! \n",
      "Okay actually, gas wise, idk, finding safe places to park for extended periods of time, it's werid. You'd be surprised how much you stand out during everyone panicking staying at home at night.\n",
      "Yet I also kinda like it. \n",
      "This motel would be $500 bucks for two weeks, maybe I can scrap the car for the room.\n",
      "You know I remember back when I still had savings $500 was like not a big deal. Don't get me wrong $500 ain't nothing to sneeze at, but that was back in like June.\n",
      "I'm seriously considering scraping the car, for that guarantee place to stay.\n",
      "I can always save up 2-3 months and get another one...\n",
      "Idk,\n",
      "Today, I'm actually less sad.\n",
      "But I guess this is the first time in like two weeks I could really analyze my situation, with rational thought.\n",
      "Anyhow, tomorrow!!!!!!! We gotta get back on the hunt, maybe find some random gig, and I'd like to pay back that loan back early.\n",
      "Easier to do work when you can eat!\n",
      "March 27Th Please Hurry!\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(X_train))\n",
    "print(type(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_pipeline_lr = Pipeline([('tfidf',TfidfVectorizer()), ('lr',LogisticRegression())])\n",
    "\n",
    "#trian the model\n",
    "model_pipeline_lr.fit(X_train, Y_train)\n",
    "\n",
    "#Predict on the test set\n",
    "y_pred_lr = model_pipeline_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "accuracy_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "precision_lr = precision_score(Y_test, y_pred_lr, average='weighted')\n",
    "recall_lr = recall_score(Y_test, y_pred_lr, average='weighted')\n",
    "f1_score_lr = f1_score(Y_test,y_pred_lr, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Logistic Regression Model\n",
      "------------------------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.61      0.78      0.69       455\n",
      "not depression       0.64      0.55      0.60       363\n",
      "        severe       0.71      0.06      0.11        82\n",
      "\n",
      "      accuracy                           0.62       900\n",
      "     macro avg       0.66      0.47      0.47       900\n",
      "  weighted avg       0.63      0.62      0.60       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_lr = classification_report(Y_test,y_pred_lr)\n",
    "\n",
    "print(\"Evaluation Metrics for Logistic Regression Model\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(classification_report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_pipeline_svm = Pipeline([('tfidf',TfidfVectorizer()), ('svm',SVC())])\n",
    "\n",
    "#trian the model\n",
    "model_pipeline_svm.fit(X_train, Y_train)\n",
    "\n",
    "#Predict on the test set\n",
    "y_pred_svm = model_pipeline_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for SVM Model\n",
      "--------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.61      0.80      0.69       455\n",
      "not depression       0.66      0.54      0.59       363\n",
      "        severe       0.60      0.04      0.07        82\n",
      "\n",
      "      accuracy                           0.62       900\n",
      "     macro avg       0.62      0.46      0.45       900\n",
      "  weighted avg       0.63      0.62      0.59       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_svm = classification_report(Y_test,y_pred_svm)\n",
    "\n",
    "print(\"Evaluation Metrics for SVM Model\")\n",
    "print(\"--------------------------------\")\n",
    "print(classification_report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_pipeline_dt = Pipeline([('tfidf', TfidfVectorizer()),('dt', DecisionTreeClassifier())])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline_dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_dt = model_pipeline_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Decision Tree Model\n",
      "------------------------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.59      0.61      0.60       455\n",
      "not depression       0.53      0.52      0.52       363\n",
      "        severe       0.27      0.26      0.26        82\n",
      "\n",
      "      accuracy                           0.54       900\n",
      "     macro avg       0.46      0.46      0.46       900\n",
      "  weighted avg       0.54      0.54      0.54       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_dt = classification_report(Y_test, y_pred_dt)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics for Decision Tree Model\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(classification_report_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_pipeline_rf = Pipeline([('tfidf', TfidfVectorizer()),('rf', RandomForestClassifier())])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = model_pipeline_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Random Forest Model\n",
      "------------------------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.59      0.78      0.68       455\n",
      "not depression       0.64      0.53      0.58       363\n",
      "        severe       0.50      0.01      0.02        82\n",
      "\n",
      "      accuracy                           0.61       900\n",
      "     macro avg       0.58      0.44      0.43       900\n",
      "  weighted avg       0.60      0.61      0.58       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_rf = classification_report(Y_test, y_pred_rf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics for Random Forest Model\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(classification_report_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev_pid_1</td>\n",
       "      <td>enjoyed today still Tomorrows depression wait ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev_pid_2</td>\n",
       "      <td>sorta tried kill total breakdown fucking car p...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev_pid_3</td>\n",
       "      <td>Best suicide method like quick easy deformitie...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dev_pid_4</td>\n",
       "      <td>story remember time 'd get 3DS play Nintendogs...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev_pid_5</td>\n",
       "      <td>world cares beautiful people 'm born ugly 've ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PID                                             Review     Label\n",
       "0  dev_pid_1  enjoyed today still Tomorrows depression wait ...  moderate\n",
       "1  dev_pid_2  sorta tried kill total breakdown fucking car p...  moderate\n",
       "2  dev_pid_3  Best suicide method like quick easy deformitie...  moderate\n",
       "3  dev_pid_4  story remember time 'd get 3DS play Nintendogs...  moderate\n",
       "4  dev_pid_5  world cares beautiful people 'm born ugly 've ...  moderate"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(['.', ',', '!', '?', ';', ':'])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "  return ' '.join(filtered_tokens)\n",
    "\n",
    "df['Review'] = df['Review'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.Review\n",
    "Y = df.Label\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_pipeline_lr = Pipeline([('tfidf',TfidfVectorizer()), ('lr',LogisticRegression())])\n",
    "\n",
    "#trian the model\n",
    "model_pipeline_lr.fit(X_train, Y_train)\n",
    "\n",
    "#Predict on the test set\n",
    "y_pred_lr = model_pipeline_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Logistic Regression Model\n",
      "------------------------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.61      0.77      0.68       455\n",
      "not depression       0.63      0.55      0.59       363\n",
      "        severe       0.73      0.10      0.17        82\n",
      "\n",
      "      accuracy                           0.62       900\n",
      "     macro avg       0.66      0.47      0.48       900\n",
      "  weighted avg       0.63      0.62      0.60       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report_lr = classification_report(Y_test,y_pred_lr)\n",
    "\n",
    "print(\"Evaluation Metrics for Logistic Regression Model\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(classification_report_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_pipeline_svm = Pipeline([('tfidf',TfidfVectorizer()), ('svm',SVC())])\n",
    "\n",
    "#trian the model\n",
    "model_pipeline_svm.fit(X_train, Y_train)\n",
    "\n",
    "#Predict on the test set\n",
    "y_pred_svm = model_pipeline_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for SVM Model\n",
      "--------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.60      0.80      0.68       455\n",
      "not depression       0.64      0.50      0.56       363\n",
      "        severe       0.62      0.06      0.11        82\n",
      "\n",
      "      accuracy                           0.61       900\n",
      "     macro avg       0.62      0.45      0.45       900\n",
      "  weighted avg       0.62      0.61      0.58       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_svm = classification_report(Y_test,y_pred_svm)\n",
    "\n",
    "print(\"Evaluation Metrics for SVM Model\")\n",
    "print(\"--------------------------------\")\n",
    "print(classification_report_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_pipeline_dt = Pipeline([('tfidf', TfidfVectorizer()),('dt', DecisionTreeClassifier())])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline_dt.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_dt = model_pipeline_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Decision Tree Model\n",
      "------------------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.59      0.55      0.57       455\n",
      "not depression       0.53      0.58      0.55       363\n",
      "        severe       0.26      0.23      0.25        82\n",
      "\n",
      "      accuracy                           0.54       900\n",
      "     macro avg       0.46      0.46      0.46       900\n",
      "  weighted avg       0.53      0.54      0.53       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_dt = classification_report(Y_test, y_pred_dt)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics for Decision Tree Model\")\n",
    "print(\"------------------------------------------\")\n",
    "print(classification_report_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_pipeline_rf = Pipeline([('tfidf', TfidfVectorizer()),('rf', RandomForestClassifier())])\n",
    "\n",
    "# Train the model\n",
    "model_pipeline_rf.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = model_pipeline_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Random Forest Model\n",
      "------------------------------------------\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "      moderate       0.60      0.80      0.68       455\n",
      "not depression       0.65      0.52      0.58       363\n",
      "        severe       0.50      0.01      0.02        82\n",
      "\n",
      "      accuracy                           0.61       900\n",
      "     macro avg       0.58      0.44      0.43       900\n",
      "  weighted avg       0.61      0.61      0.58       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_rf = classification_report(Y_test, y_pred_rf)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics for Random Forest Model\")\n",
    "print(\"------------------------------------------\")\n",
    "print(classification_report_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
